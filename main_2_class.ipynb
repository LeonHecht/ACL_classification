{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing 2 classifications in a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Start-Time\")\n",
    "# print current time in format: 2019-10-03 13:10:00\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'distilbert-base-uncased'\n",
    "# model = 'roberta-base'\n",
    "# model = 'bert-large-uncased'\n",
    "# model = 'xlnet-large-cased'\n",
    "model = 'xlm-roberta-large'\n",
    "# model = 'microsoft/deberta-v2-xxlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in words if word.isalnum()]\n",
    "\n",
    "def filter_sentences(row):\n",
    "    # Assuming keywords are separated by commas and possibly spaces\n",
    "    keywords = [lemmatizer.lemmatize(word) for word in row['keyword'].replace(' ', '').split(',')]\n",
    "    text = row['text']\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Filter sentences that contain at least one lemmatized keyword\n",
    "    filtered_sentences = set()  # Use a set to prevent duplicates\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        words = lemmatize_words(word_tokenize(sentence))\n",
    "        if any(keyword in words for keyword in keywords):\n",
    "            # Add previous sentence if it exists\n",
    "            if index > 0:\n",
    "                filtered_sentences.add(sentences[index - 1])\n",
    "            # Add current sentence\n",
    "            filtered_sentences.add(sentence)\n",
    "            # Add next sentence if it exists\n",
    "            if index < len(sentences) - 1:\n",
    "                filtered_sentences.add(sentences[index + 1])\n",
    "\n",
    "    return ' '.join(sorted(filtered_sentences)) if filtered_sentences else text  # Return original text if no keywords found\n",
    "\n",
    "\n",
    "print(\"Reading data...\")\n",
    "df = pd.read_csv('data/SMM4H_2024_Task3_Training_1800.csv', usecols=['id', 'keyword', 'text', 'label'])\n",
    "df_val = pd.read_csv('data/SMM4H_2024_Task3_Validation_600.csv', usecols=['id', 'keyword', 'text', 'label'])\n",
    "print(\"Data read...\")\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply the function to filter sentences in the text\n",
    "df['text'] = df.apply(filter_sentences, axis=1)\n",
    "df_val['text'] = df_val.apply(filter_sentences, axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get separation token by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sep_token(model):\n",
    "    if model == 'distilbert-base-uncased' or model == 'roberta-base' or model == 'bert-large-uncased' or model == 'microsoft/deberta-v2-xxlarge':\n",
    "        sep_token = '[SEP]'\n",
    "    elif model == 'xlnet-large-cased':\n",
    "        sep_token = '<sep>'\n",
    "    elif model == 'xlm-roberta-large':\n",
    "        sep_token = '</s>'\n",
    "    return sep_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_keywords(df_, model):\n",
    "    sep_token = get_sep_token(model)\n",
    "    \n",
    "    df_['text'] = df_['text'] + f\" {sep_token} Keyword: \" + df_['keyword']\n",
    "    df_.drop(columns=['keyword'], inplace=True)\n",
    "    return df_\n",
    "\n",
    "df = add_keywords(df, model)\n",
    "df_val = add_keywords(df_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import emoji library\n",
    "import emoji\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    # Perform emoji to text conversion\n",
    "    text = emoji.demojize(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    # text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df_val['text'] = df_val['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get df for Classification 1: 0 vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class 1, 2, 3 to 1\n",
    "df['labels_mapped_0_1'] = df['label'].apply(lambda x: 1 if x in [1, 2, 3] else 0)\n",
    "df_val['labels_mapped_0_1'] = df_val['label'].apply(lambda x: 1 if x in [1, 2, 3] else 0)\n",
    "\n",
    "# print class count\n",
    "print(df['labels_mapped_0_1'].value_counts())\n",
    "print(df_val['labels_mapped_0_1'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data for first classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_texts, val_texts, y_train, y_val = train_test_split(\n",
    "    df['text'], df['labels_mapped_0_1'],\n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "test_texts = df_val['text']\n",
    "y_test = df_val['labels_mapped_0_1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the training texts and labels\n",
    "train_df = pd.DataFrame({'text': train_texts, 'label': y_train})\n",
    "\n",
    "# Contar el número de publicaciones en cada categoría\n",
    "class_counts = train_df['label'].value_counts()\n",
    "print(\"Class distribution before augmenting with paraphrased texts:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment train_df by paraphased texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_1_df_1 = pd.read_csv('data/augmented_train_dfs/Paraphrase1/paraphrased_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
    "paraphrased_1_df_2 = pd.read_csv('data/augmented_train_dfs/Paraphrase2/paraphrased_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
    "\n",
    "paraphrased_2_df_1 = pd.read_csv('data/augmented_train_dfs/Paraphrase1/paraphrased_class_2.csv', usecols=['text', 'label', 'keyword'])\n",
    "\n",
    "paraphrased_3_df_1 = pd.read_csv('data/augmented_train_dfs/Paraphrase1/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
    "paraphrased_3_df_2 = pd.read_csv('data/augmented_train_dfs/Paraphrase2/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
    "paraphrased_3_df_3 = pd.read_csv('data/augmented_train_dfs/Paraphrase3/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
    "\n",
    "paraphrased_df = pd.concat([paraphrased_1_df_1, paraphrased_1_df_2, paraphrased_2_df_1, paraphrased_3_df_1, paraphrased_3_df_2, paraphrased_3_df_3], ignore_index=True)\n",
    "paraphrased_df['label'] = 1\n",
    "\n",
    "# Add keywords to paraphrased dfs\n",
    "paraphrased_df = add_keywords(paraphrased_df, model)\n",
    "\n",
    "# # Add sentiment to paraphrased dfs\n",
    "# # Apply the function to get the compound sentiment score for each post\n",
    "# paraphrased_df['vader_sentiment'] = paraphrased_df['text'].apply(get_vader_sentiment)\n",
    "# paraphrased_df = add_vader_sentiment(paraphrased_df, model)\n",
    "\n",
    "# merge df with paraphrased dfs\n",
    "train_df = pd.concat([train_df, paraphrased_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract texts and labels from train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# Now you can extract the texts and labels\n",
    "train_texts = shuffled_train_df['text']\n",
    "print(\"Train texts balanced\", train_texts)\n",
    "# print datatype of y train values\n",
    "y_train = shuffled_train_df['label']\n",
    "print(\"Datatype of y_train\", type(y_train))\n",
    "print(\"y_train balanced\", y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print train_df class distribution after cutting/before augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar el número de publicaciones en cada categoría\n",
    "class_counts = train_df['label'].value_counts()\n",
    "print(\"Class distribution after cutting:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import tune_transformer_GPU_0\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Model:\", model)\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"Converting train, val and test texts to csv...\")\n",
    "train_texts.to_csv('data/train_texts.csv', index=False, header=False)\n",
    "val_texts.to_csv('data/val_texts.csv', index=False, header=False)\n",
    "test_texts.to_csv('data/test_texts.csv', index=False, header=False)\n",
    "\n",
    "test_pred_labels = tune_transformer_GPU_0.run_optimization(model, 2, train_texts, val_texts, test_texts, y_train, y_val, y_test)\n",
    "\n",
    "# replace original test labels with predicted labels\n",
    "df_val['label_pred_1'] = test_pred_labels\n",
    "\n",
    "# # save the dataframe with predicted labels to a csv file\n",
    "# print(\"Saving predictions to csv...\")\n",
    "# df_val.to_csv('data/prediction_task3.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print End Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"End-Time\")\n",
    "# print current time in format: 2019-10-03 13:10:00\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2nd classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for 2nd Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'distilbert-base-uncased'\n",
    "# model = 'roberta-base'\n",
    "# model = 'bert-large-uncased'\n",
    "model = 'xlnet-large-cased'\n",
    "# model = 'xlm-roberta-large'\n",
    "# model = 'microsoft/deberta-v2-xxlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create 2nd df dropping the class 0\n",
    "# df_2 = df[df['labels_mapped_0_1'] == 1].copy()\n",
    "# # create 2nd df_val dropping the class 0 concerning 'label_pred_1' (the predictions already made)\n",
    "# df_val_2 = df_val[df_val['labels_mapped_0_1'] == 1].copy()\n",
    "\n",
    "# print(\"Before remapping labels:\")\n",
    "# print(df_2['label'].value_counts())\n",
    "# print(df_val_2['label'].value_counts())\n",
    "\n",
    "# remapped_labels = {1: 0, 2: 1, 3: 2}\n",
    "\n",
    "# print(\"df_val['label']\\n\", df_val['label'])\n",
    "\n",
    "# df_2['label'] = df_2['label'].map(remapped_labels).astype(int)\n",
    "# df_val_2['label'] = df_val_2['label'].map(remapped_labels).astype(int)\n",
    "\n",
    "# print(\"After remapping labels:\")\n",
    "# print(df_2['label'].value_counts())\n",
    "# print(df_val_2['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data for 2nd classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_texts, val_texts, y_train, y_val = train_test_split(\n",
    "    df.loc[df['labels_mapped_0_1'] == 1, 'text'], \n",
    "    df.loc[df['labels_mapped_0_1'] == 1, 'label'],\n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "test_texts = df_val.loc[df_val['label_pred_1'] == 1, 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the training texts and labels\n",
    "train_df = pd.DataFrame({'text': train_texts, 'label': y_train})\n",
    "\n",
    "# Contar el número de publicaciones en cada categoría\n",
    "class_counts = train_df['label'].value_counts()\n",
    "print(\"Class distribution before augmenting with paraphrased texts:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment train_df by paraphased texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_1_df_1 = pd.read_csv('data/augmented_train_dfs/Paraphrase1/paraphrased_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
    "paraphrased_1_df_2 = pd.read_csv('data/augmented_train_dfs/Paraphrase2/paraphrased_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
    "\n",
    "paraphrased_2_df_1 = pd.read_csv('data/augmented_train_dfs/Paraphrase1/paraphrased_class_2.csv', usecols=['text', 'label', 'keyword'])\n",
    "\n",
    "paraphrased_3_df_1 = pd.read_csv('data/augmented_train_dfs/Paraphrase1/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
    "paraphrased_3_df_2 = pd.read_csv('data/augmented_train_dfs/Paraphrase2/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
    "paraphrased_3_df_3 = pd.read_csv('data/augmented_train_dfs/Paraphrase3/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
    "\n",
    "paraphrased_df = pd.concat([paraphrased_1_df_1, paraphrased_1_df_2, paraphrased_2_df_1, paraphrased_3_df_1, paraphrased_3_df_2, paraphrased_3_df_3], ignore_index=True)\n",
    "\n",
    "# Add keywords to paraphrased dfs\n",
    "paraphrased_df = add_keywords(paraphrased_df, model)\n",
    "\n",
    "# # Add sentiment to paraphrased dfs\n",
    "# # Apply the function to get the compound sentiment score for each post\n",
    "# paraphrased_df['vader_sentiment'] = paraphrased_df['text'].apply(get_vader_sentiment)\n",
    "# paraphrased_df = add_vader_sentiment(paraphrased_df, model)\n",
    "\n",
    "# merge df with paraphrased dfs\n",
    "train_df = pd.concat([train_df, paraphrased_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract texts and labels from train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# Now you can extract the texts and labels\n",
    "train_texts = shuffled_train_df['text']\n",
    "print(\"Train texts balanced\", train_texts)\n",
    "# print datatype of y train values\n",
    "y_train = shuffled_train_df['label']\n",
    "print(\"Datatype of y_train\", type(y_train))\n",
    "print(\"y_train balanced\", y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remap labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_labels = {1: 0, 2: 1, 3: 2}\n",
    "y_train = y_train.map(remapped_labels).astype(int)\n",
    "y_val = y_val.map(remapped_labels).astype(int)\n",
    "# y_test = y_test.map(remapped_labels).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import tune_transformer_GPU_0\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Model:\", model)\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"Converting train, val and test texts to csv...\")\n",
    "train_texts.to_csv('data/train_texts.csv', index=False, header=False)\n",
    "val_texts.to_csv('data/val_texts.csv', index=False, header=False)\n",
    "test_texts.to_csv('data/test_texts.csv', index=False, header=False)\n",
    "\n",
    "test_pred_labels = tune_transformer_GPU_0.run_optimization(model, 3, train_texts, val_texts, test_texts, y_train, y_val, None)\n",
    "\n",
    "print(\"type test_pred_labels:\", type(test_pred_labels))\n",
    "remapped_labels2 = {0: 1, 1: 2, 2: 3}\n",
    "test_pred_labels = pd.Series(test_pred_labels)\n",
    "test_pred_labels = test_pred_labels.map(remapped_labels2).astype(int)\n",
    "\n",
    "print(\"Len test_pred_labels:\", len(test_pred_labels))\n",
    "print(\"Len df_val['label']:\", len(df_val['label']))\n",
    "print(\"Len df_val['label_pred_1']:\", len(df_val['label_pred_1']))\n",
    "\n",
    "# replace original test labels with predicted labels\n",
    "df_val.loc[df_val['label_pred_1'] == 1, 'label_pred_1'] = test_pred_labels.values\n",
    "\n",
    "print(\"Info on df_val['label_pred_1']:\\n\", df_val['label_pred_1'].value_counts())\n",
    "\n",
    "# print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_val['label'], df_val['label_pred_1']))\n",
    "\n",
    "# # save the dataframe with predicted labels to a csv file\n",
    "# print(\"Saving predictions to csv...\")\n",
    "# df_val.to_csv('data/prediction_task3.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Convert 'rest' predictions to None\n",
    "# combined_predictions = ['0' if prediction == 0 else None for prediction in df_val['label_pred_1']]\n",
    "\n",
    "# # Step 2: Map 'rest' indices to the second step predictions\n",
    "# rest_indices = [i for i, prediction in enumerate(combined_predictions) if prediction is None]\n",
    "# for i, prediction in zip(rest_indices, df_val_2['label_pred_2']):\n",
    "#     combined_predictions[i] = prediction  # Map to the original labels for class 1, 2, and 3\n",
    "\n",
    "# # Step 3: Update the DataFrame\n",
    "# df['final_predictions'] = combined_predictions\n",
    "\n",
    "# # Print classficiation report based on df['final_predictions'] and df['label']\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(df_val['label'], df['final_predictions'], target_names=['0', '1', '2', '3']))\n",
    "\n",
    "# Inicializar la columna final_prediction con las predicciones de la primera fase\n",
    "# df_val['final_prediction'] = df_val['label_pred_1']\n",
    "\n",
    "# print(\"Info on df_val['final_prediction']:\\n\", df_val['final_prediction'].value_counts())\n",
    "\n",
    "# # Actualizar las predicciones de la segunda fase en la columna final_prediction\n",
    "# df_val[df_val['labels_mapped_0_1'] == 1]['final_prediction'] = df_val[df_val['labels_mapped_0_1'] == 1]['label_pred_2']\n",
    "\n",
    "# print(\"Info on df_val['final_prediction']:\\n\", df_val['final_prediction'].value_counts())\n",
    "\n",
    "# # Print classficiation report based on df['final_predictions'] and df['label']\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(df_val['label'], df_val['final_prediction'], target_names=['0', '1', '2', '3']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
