{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing 2 classifications in a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Start-Time\n",
      "2024-04-17 18:47:57\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Start-Time\")\n",
    "# print current time in format: 2019-10-03 13:10:00\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'distilbert-base-uncased'\n",
    "# model = 'roberta-base'\n",
    "# model = 'bert-large-uncased'\n",
    "# model = 'xlnet-large-cased'\n",
    "model = 'xlm-roberta-large'\n",
    "# model = 'microsoft/deberta-v2-xxlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data read...\n",
      "           id                                keyword  \\\n",
      "0      3u2w5k                                    run   \n",
      "1      3xbury                                outside   \n",
      "2      3y743u                       run, swim, climb   \n",
      "3      43bvs7                                   walk   \n",
      "4      442ap2                                outside   \n",
      "...       ...                                    ...   \n",
      "1795   gqzye9                     pool, beach,  pool   \n",
      "1796   env299                     outside , outdoors   \n",
      "1797  e9bnr1s                                Jogging   \n",
      "1798   qrmhbe                  walk, swimming,  pool   \n",
      "1799   mxbsm8  roller blade, outside , roller blades   \n",
      "\n",
      "                                                   text  label  \n",
      "0     Afterwards, I want to make a run at young love...      0  \n",
      "1     I've met her and several other girls on a spec...      0  \n",
      "2     Anyway! I'd also like to be able to talk to pe...      0  \n",
      "3     I did and again I failed to utter a word. I go...      0  \n",
      "4     Anyway I ran back inside sort of grunting to a...      0  \n",
      "...                                                 ...    ...  \n",
      "1795  Essentially lived like a victorian woman in 18...      3  \n",
      "1796  But I don't go outside much. I mean I'm loud a...      3  \n",
      "1797  If you look for the light, you will find it. I...      3  \n",
      "1798  I did feel anxiety and some strong emotions bu...      3  \n",
      "1799  As Iâ€™m rolling, I hear a man in the parking lo...      3  \n",
      "\n",
      "[1800 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in words if word.isalnum()]\n",
    "\n",
    "def filter_sentences(row):\n",
    "    # Assuming keywords are separated by commas and possibly spaces\n",
    "    keywords = [lemmatizer.lemmatize(word) for word in row['keyword'].replace(' ', '').split(',')]\n",
    "    text = row['text']\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Filter sentences that contain at least one lemmatized keyword\n",
    "    filtered_sentences = set()  # Use a set to prevent duplicates\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        words = lemmatize_words(word_tokenize(sentence))\n",
    "        if any(keyword in words for keyword in keywords):\n",
    "            # Add previous sentence if it exists\n",
    "            if index > 0:\n",
    "                filtered_sentences.add(sentences[index - 1])\n",
    "            # Add current sentence\n",
    "            filtered_sentences.add(sentence)\n",
    "            # Add next sentence if it exists\n",
    "            if index < len(sentences) - 1:\n",
    "                filtered_sentences.add(sentences[index + 1])\n",
    "\n",
    "    return ' '.join(sorted(filtered_sentences)) if filtered_sentences else text  # Return original text if no keywords found\n",
    "\n",
    "\n",
    "print(\"Reading data...\")\n",
    "df = pd.read_csv('data/SMM4H_2024_Task3_Training_1800.csv', usecols=['id', 'keyword', 'text', 'label'])\n",
    "df_val = pd.read_csv('data/SMM4H_2024_Task3_Validation_600.csv', usecols=['id', 'keyword', 'text', 'label'])\n",
    "print(\"Data read...\")\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply the function to filter sentences in the text\n",
    "df['text'] = df.apply(filter_sentences, axis=1)\n",
    "df_val['text'] = df_val.apply(filter_sentences, axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_keywords(df_, model):\n",
    "    if model == 'distilbert-base-uncased' or model == 'roberta-base' or model == 'bert-large-uncased' or model == 'microsoft/deberta-v2-xxlarge':\n",
    "        sep_token = '[SEP]'\n",
    "    elif model == 'xlnet-large-cased':\n",
    "        sep_token = '<sep>'\n",
    "    elif model == 'xlm-roberta-large':\n",
    "        sep_token = '</s>'\n",
    "    \n",
    "    df_['text'] = df_['text'] + f\" {sep_token} \" + df_['keyword']\n",
    "    df_.drop(columns=['keyword'], inplace=True)\n",
    "    return df_\n",
    "\n",
    "df = add_keywords(df, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import emoji library\n",
    "import emoji\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    # Perform emoji to text conversion\n",
    "    text = emoji.demojize(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    # text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get df for Classification 1: 0 vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_1\n",
      "0    1131\n",
      "1     669\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Map class 1, 2, 3 to 1\n",
    "df['label_1'] = df['label'].apply(lambda x: 1 if x in [1, 2, 3] else 0)\n",
    "df_val['label_1'] = df_val['label'].apply(lambda x: 1 if x in [1, 2, 3] else 0)\n",
    "\n",
    "# print class count\n",
    "print(df['label_1'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data for first classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_texts, val_texts, y_train, y_val = train_test_split(\n",
    "    df['text'], df['label_1'],\n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "test_texts = df_val['text']\n",
    "y_test = df_val['label_1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tune_transformer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n",
      "File \u001b[1;32mg:\\Mi unidad\\UNAM\\2024-II\\ACL_classification\\models\\tune_transformer.py:2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, TrainerControl, TrainerState\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from models import tune_transformer\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Model:\", model)\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"Converting train, val and test texts to csv...\")\n",
    "train_texts.to_csv('data/train_texts.csv', index=False, header=False)\n",
    "val_texts.to_csv('data/val_texts.csv', index=False, header=False)\n",
    "test_texts.to_csv('data/test_texts.csv', index=False, header=False)\n",
    "\n",
    "test_pred_labels = tune_transformer.run(model, 2, train_texts, val_texts, test_texts, y_train, y_val, y_test)\n",
    "\n",
    "# replace original test labels with predicted labels\n",
    "df_val['label_pred'] = test_pred_labels\n",
    "\n",
    "# # save the dataframe with predicted labels to a csv file\n",
    "# print(\"Saving predictions to csv...\")\n",
    "# df_val.to_csv('data/prediction_task3.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print End Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"End-Time\")\n",
    "# print current time in format: 2019-10-03 13:10:00\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2nd classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for 2nd Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'distilbert-base-uncased'\n",
    "# model = 'roberta-base'\n",
    "# model = 'bert-large-uncased'\n",
    "model = 'xlnet-large-cased'\n",
    "# model = 'xlm-roberta-large'\n",
    "# model = 'microsoft/deberta-v2-xxlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2nd df dropping the class 0\n",
    "df_2 = df[df['label_1'] == 1]\n",
    "\n",
    "# create 2nd df_val dropping the class 0 concerning 'label_pred'\n",
    "df_val_2 = df_val[df_val['label_pred'] == 1]\n",
    "\n",
    "print(df_2['label'].value_counts())\n",
    "\n",
    "remapped_labels = {1: 0, 2: 1, 3: 2}\n",
    "\n",
    "df_2['label'] = df_2['label'].map(remapped_labels)\n",
    "df_val_2['label'] = df_val_2['label'].map(remapped_labels)\n",
    "\n",
    "print(df_2['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data for 2nd classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_texts, val_texts, y_train, y_val = train_test_split(\n",
    "    df_2['text'], df_2['label'],\n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "test_texts = df_val_2['text']\n",
    "y_test = df_val_2['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import tune_transformer\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Model:\", model)\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"Converting train, val and test texts to csv...\")\n",
    "train_texts.to_csv('data/train_texts.csv', index=False, header=False)\n",
    "val_texts.to_csv('data/val_texts.csv', index=False, header=False)\n",
    "test_texts.to_csv('data/test_texts.csv', index=False, header=False)\n",
    "\n",
    "test_pred_labels = tune_transformer.run(model, 3, train_texts, val_texts, test_texts, y_train, y_val, y_test)\n",
    "\n",
    "remapped_labels2 = {0: 1, 1: 2, 2: 3}\n",
    "# replace original test labels with predicted labels\n",
    "df_val_2['label_pred2'] = test_pred_labels\n",
    "df_val_2['label_pred2'] = df_val_2['label_pred2'].map(remapped_labels2)\n",
    "\n",
    "# # save the dataframe with predicted labels to a csv file\n",
    "# print(\"Saving predictions to csv...\")\n",
    "# df_val.to_csv('data/prediction_task3.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert 'rest' predictions to None\n",
    "combined_predictions = ['0' if prediction == 0 else None for prediction in df_val['label_pred']]\n",
    "\n",
    "# Step 2: Map 'rest' indices to the second step predictions\n",
    "rest_indices = [i for i, prediction in enumerate(combined_predictions) if prediction is None]\n",
    "for i, prediction in zip(rest_indices, df_val_2['label_pred2']):\n",
    "    combined_predictions[i] = prediction  # Map to the original labels for class 1, 2, and 3\n",
    "\n",
    "# Step 3: Update the DataFrame\n",
    "df['predictions'] = combined_predictions\n",
    "\n",
    "# Print classficiation report based on df['predictions'] and df['label']\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_val['label'], df['predictions'], target_names=['0', '1', '2', '3']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
