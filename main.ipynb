{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print Start time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------\n",
            "Start-Time\n",
            "2024-04-01 21:22:42\n",
            "------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "print(\"------------------------------------------------\")\n",
        "print(\"Start-Time\")\n",
        "# print current time in format: 2019-10-03 13:10:00\n",
        "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
        "print(\"------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Does df-save-path exist: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Does df-save-path exist:\", os.path.exists('data/augmented_train_dfs'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify deploying mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "deploying = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = 'DistilBert'\n",
        "# model = 'RoBERTa'\n",
        "# model = 'Bert-Large'\n",
        "model = 'XLNet-Large'\n",
        "# model = 'XLM-Roberta-Large'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution of val df:\n",
            " label\n",
            "0    377\n",
            "2    131\n",
            "1     54\n",
            "3     38\n",
            "Name: count, dtype: int64\n",
            "               keyword                                               text  \\\n",
            "0                  run  21/m. I want to experience young love, but I'v...   \n",
            "1              outside  Having issues talking to a girl whom I enjoyed...   \n",
            "2     run, swim, climb  Need some advice for free social activities. I...   \n",
            "3                 walk   I spoke to her today.. A few weeks ago I met ...   \n",
            "4              outside  How to get over Social Anxiety?. Hello, The ot...   \n",
            "...                ...                                                ...   \n",
            "2395              walk   The thought of applying for a job terrified m...   \n",
            "2396           outside   No one will ever love me because I don't have...   \n",
            "2397     outside, walk  5 or 7, I never shit outside of my house. Once...   \n",
            "2398           outside   Yes, I hate eating in front of anyone I don’t...   \n",
            "2399           outside   I’ve always experienced this sensation of bei...   \n",
            "\n",
            "      label  \n",
            "0         0  \n",
            "1         0  \n",
            "2         0  \n",
            "3         0  \n",
            "4         0  \n",
            "...     ...  \n",
            "2395      2  \n",
            "2396      2  \n",
            "2397      2  \n",
            "2398      2  \n",
            "2399      2  \n",
            "\n",
            "[2400 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"Reading data...\")\n",
        "df = pd.read_csv('data/SMM4H_2024_Task3_Training_1800.csv', usecols=['id', 'keyword', 'text', 'label'])\n",
        "df_val = pd.read_csv('data/SMM4H_2024_Task3_Validation_600.csv', usecols=['id', 'keyword', 'text', 'label'])\n",
        "print(\"Data read...\")\n",
        "# Keep the validation data apart when deploying\n",
        "if not deploying:\n",
        "    df = pd.concat([df, df_val], ignore_index=True)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify augmentation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmenting = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add Keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not augmenting:\n",
        "    if model == 'DistilBert' or model == 'RoBERTa' or model == 'Bert-Large':\n",
        "        sep_token = '[SEP]'\n",
        "    elif model == 'XLNet-Large':\n",
        "        sep_token = '<sep>'\n",
        "    elif model == 'XLM-Roberta-Large':\n",
        "        sep_token = '</s>'\n",
        "    \n",
        "    df['text'] = df['text'] + f\" {sep_token} \" + df['keyword']\n",
        "    df.drop(columns=['keyword'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQTbkPUSp-Zk"
      },
      "source": [
        "## Clean text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6qh9SyUqAHJ"
      },
      "outputs": [],
      "source": [
        "# import emoji library\n",
        "import emoji\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    import re\n",
        "    # Perform emoji to text conversion\n",
        "    text = emoji.demojize(text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "    # Remove special characters and numbers\n",
        "    # text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CXNHCTPGiBr"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "if not augmenting and not deploying:\n",
        "    # ----- Without Keywords (for training) -----\n",
        "    # First, split the data into a training set and a temporary set (which will be further split into validation and test sets)\n",
        "    train_ids, temp_ids, train_texts, temp_texts, y_train, temp_labels = train_test_split(\n",
        "        df['id'], df['text'], df['label'],\n",
        "        test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Next, split the temporary set into validation and test sets\n",
        "    val_ids, test_ids, val_texts, test_texts, y_val, y_test = train_test_split(\n",
        "        temp_ids, temp_texts, temp_labels,\n",
        "        test_size=0.5, random_state=42\n",
        "    )\n",
        "\n",
        "elif augmenting:\n",
        "    # ----- With Keywords (for augmenting) -----\n",
        "    # First, split the data into a training set and a temporary set (which will be further split into validation and test sets)\n",
        "    train_texts, temp_texts, train_keywords, temp_keywords, y_train, temp_labels = train_test_split(\n",
        "        df['text'], df['keyword'], df['label'], test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Next, split the temporary set into validation and test sets\n",
        "    val_texts, test_texts, val_keywords, test_keywords, y_val, y_test = train_test_split(\n",
        "        temp_texts, temp_keywords, temp_labels, test_size=0.5, random_state=42\n",
        "    )\n",
        "    \n",
        "elif deploying:\n",
        "    train_ids, val_ids, train_texts, val_texts, y_train, y_val = train_test_split(\n",
        "        df['id'], df['text'], df['label'],\n",
        "        test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    test_texts = df_val['text']\n",
        "    y_test = df_val['label']\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# df_plot = df.copy()\n",
        "\n",
        "# label_mapping = {1: 'positive', 2: 'neutral', 3: 'negative', 0: 'unrelated'}\n",
        "# df_plot['label'] = df_plot['label'].map(label_mapping)\n",
        "\n",
        "# # Contar el número de publicaciones en cada categoría\n",
        "# class_counts = df_plot['label'].value_counts()\n",
        "# print(class_counts)\n",
        "\n",
        "# # Crear un gráfico de barras\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# class_counts.plot(kind='bar')\n",
        "# plt.title('Distribución de clases')\n",
        "# plt.xlabel('Clase')\n",
        "# plt.ylabel('Número de publicaciones')\n",
        "# plt.xticks(rotation=0)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not augmenting:\n",
        "    # ----- Without Keywords (for training) -----\n",
        "    # Create a DataFrame from the training texts and labels\n",
        "    train_df = pd.DataFrame({'id': train_ids, 'text': train_texts, 'label': y_train})\n",
        "    real_id = '3u2w5k'\n",
        "    real_label = 0\n",
        "\n",
        "    real_id2 = '8ugfyp'\n",
        "    real_label2 = 3\n",
        "else:  \n",
        "    # ----- With Keywords (for augmenting) -----\n",
        "    # Create a DataFrame from the training texts and labels\n",
        "    train_df = pd.DataFrame({'text': train_texts, 'label': y_train, 'keyword': train_keywords})\n",
        "\n",
        "# Contar el número de publicaciones en cada categoría\n",
        "class_counts = train_df['label'].value_counts()\n",
        "print(\"Class distribution before cutting:\\n\", class_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augment train_df by paraphased texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not augmenting:\n",
        "    ## ----------- Comment code if augmenting data ------------\n",
        "\n",
        "    paraphrased_1_df = pd.read_csv('data/augmented_train_dfs/paraphrased_class_1.csv', usecols=['text', 'label'])\n",
        "    paraphrased_3_df = pd.read_csv('data/augmented_train_dfs/paraphrased_class_3.csv', usecols=['text', 'label'])\n",
        "\n",
        "    # merge df with paraphrased dfs\n",
        "    train_df = pd.concat([train_df, paraphrased_1_df, paraphrased_3_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augment train_df by backtranslated texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if not augmenting:\n",
        "\n",
        "    ## ----------- Comment code if augmenting data ------------\n",
        "\n",
        "    # back_translated_1_df = pd.read_csv('data/augmented_train_dfs/backtranslated_class_1.csv', usecols=['text', 'label'])\n",
        "    # back_translated_3_df = pd.read_csv('data/augmented_train_dfs/backtranslated_class_3.csv', usecols=['text', 'label'])\n",
        "\n",
        "    # # merge df with backtranslated dfs\n",
        "    # train_df = pd.concat([train_df, back_translated_1_df, back_translated_3_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cut Classes to X texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Fgq27NJvF5",
        "outputId": "aed84b8f-48cf-4880-c98b-d7c908ff93ed"
      },
      "outputs": [],
      "source": [
        "# from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "# if not augmenting:\n",
        "#     ## ----------- Comment code if augmenting data ------------\n",
        "\n",
        "#     # Sample 200 texts from each class (or as many as are available for classes with fewer than 200 examples)\n",
        "#     sampled_dfs = []\n",
        "#     for label in train_df['label'].unique():\n",
        "#         class_sample_size = min(len(train_df[train_df['label'] == label]), 181)\n",
        "#         sampled_dfs.append(train_df[train_df['label'] == label].sample(n=class_sample_size, random_state=42))\n",
        "\n",
        "#     # Concatenate the samples to create a balanced training DataFrame\n",
        "#     train_df = pd.concat(sampled_dfs, ignore_index=True)\n",
        "    \n",
        "#     # Shuffle the DataFrame\n",
        "#     train_df = shuffle(train_df, random_state=42)\n",
        "#     print(\"Train df:\\n\", train_df)\n",
        "\n",
        "#     # Assuming 'df' is your DataFrame and 'text' is the column name\n",
        "#     rows_with_phrase = train_df[train_df['text'].str.contains(\"I want to experience young love\")]\n",
        "\n",
        "#     # Get the IDs of those rows\n",
        "#     # ids = rows_with_phrase['id'].tolist()\n",
        "#     # print(\"Real ID:\", real_id)\n",
        "#     # print(\"Train_df ids\", ids)\n",
        "\n",
        "#     # labels = rows_with_phrase['label'].tolist()\n",
        "#     # print(\"Real label:\", real_label)\n",
        "#     # print(\"Train df labels\", labels)\n",
        "\n",
        "#     rows_with_phrase2 = train_df[train_df['text'].str.contains(\"I wrote this as a bit of a vent for myself\")]\n",
        "\n",
        "#     # Get the IDs of those rows\n",
        "#     ids = rows_with_phrase2['id'].tolist()\n",
        "#     print(\"Real ID:\", real_id2)\n",
        "#     print(\"Train_df ids\", ids)\n",
        "\n",
        "#     labels = rows_with_phrase2['label'].tolist()\n",
        "#     print(\"Real label:\", real_label2)\n",
        "#     print(\"Train df labels\", labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract texts and labels from train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not augmenting:\n",
        "    shuffled_train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    # Now you can extract the texts and labels\n",
        "    train_texts = shuffled_train_df['text']\n",
        "    print(\"Train texts balanced\", train_texts)\n",
        "    # print datatype of y train values\n",
        "    y_train = shuffled_train_df['label']\n",
        "    print(\"Datatype of y_train\", type(y_train))\n",
        "    print(\"y_train balanced\", y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print train_df class distribution after cutting/before augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar el número de publicaciones en cada categoría\n",
        "class_counts = train_df['label'].value_counts()\n",
        "print(\"Class distribution after cutting:\\n\", class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# df_plot = train_df.copy()\n",
        "\n",
        "# label_mapping = {1: 'positive', 2: 'neutral', 3: 'negative', 0: 'unrelated'}\n",
        "# df_plot['label'] = df_plot['label'].map(label_mapping)\n",
        "\n",
        "# # Contar el número de publicaciones en cada categoría\n",
        "# class_counts = df_plot['label'].value_counts()\n",
        "# print(class_counts)\n",
        "\n",
        "# # Crear un gráfico de barras\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# class_counts.plot(kind='bar')\n",
        "# plt.title('Distribución de clases')\n",
        "# plt.xlabel('Clase')\n",
        "# plt.ylabel('Número de publicaciones')\n",
        "# plt.xticks(rotation=0)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backtranslate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "backtranslate = False\n",
        "# Save the augmented training dataframe to a CSV file\n",
        "train_df_path = 'data/augmented_train_dfs/train_df_plus_backtranslated_class_1_3.csv'\n",
        "\n",
        "if backtranslate:\n",
        "\n",
        "    from utils import backtranslation\n",
        "\n",
        "    for label in {1, 3}:\n",
        "        print(f\"Backtranslating class {label}...\")\n",
        "        # Backtranslate and augment the data for underrepresented classes\n",
        "        selected_texts = train_df[train_df['label'] == label]['text']\n",
        "        selected_keywords = train_df[train_df['label'] == label]['keyword']\n",
        "        print(f\"length texts of label {label}\", len(selected_texts))\n",
        "        augmented_texts = backtranslation.backtranslate_t5(selected_texts.to_list(), selected_keywords.to_list())\n",
        "        augmented_df = pd.DataFrame({'text': augmented_texts, 'label': [label] * len(augmented_texts)})\n",
        "        augmented_df.to_csv(f'data/augmented_train_dfs/backtranslated_t5_class_{label}.csv', index=False)\n",
        "        train_df = pd.concat([train_df, augmented_df])\n",
        "\n",
        "    # Check the new class distribution after backtranslation\n",
        "    print(\"Class distribution after backtranslation:\", train_df['label'].value_counts())\n",
        "\n",
        "    train_df.to_csv(train_df_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paraphrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paraphrase = False\n",
        "# Save the augmented training dataframe to a CSV file\n",
        "# train_df_path = 'data/augmented_train_dfs/train_df_plus_paraphased_class_1_3.csv'\n",
        "\n",
        "if paraphrase:\n",
        "\n",
        "    from utils import paraphrase\n",
        "\n",
        "    for label in {1, 3}:\n",
        "        print(f\"Paraphrasing class {label}...\")\n",
        "        # Backtranslate and augment the data for underrepresented classes\n",
        "        selected_texts = train_df[train_df['label'] == label]['text']\n",
        "        selected_keywords = train_df[train_df['label'] == label]['keyword']\n",
        "        print(f\"length texts of label {label}\", len(selected_texts))\n",
        "        augmented_texts = paraphrase.paraphrase(selected_texts.to_list(), selected_keywords.to_list())\n",
        "        augmented_df = pd.DataFrame({'text': augmented_texts, 'label': [label] * len(augmented_texts)})\n",
        "        augmented_df.to_csv(f'data/augmented_train_dfs/paraphrased_class_{label}.csv', index=False)\n",
        "        train_df = pd.concat([train_df, augmented_df])\n",
        "\n",
        "    # Check the new class distribution after paraphrasing\n",
        "    print(\"Class distribution after paraphrasing:\", train_df['label'].value_counts())\n",
        "\n",
        "    # train_df.to_csv(train_df_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print train_df class distribution after augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ----------- Comment code if augmenting data ------------\n",
        "\n",
        "# # Contar el número de publicaciones en cada categoría\n",
        "# class_counts = train_df['label'].value_counts()\n",
        "# print(\"Class distribution after augmentation:\\n\", class_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models import tune_transformer\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Model:\", model)\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "if not augmenting:\n",
        "\n",
        "    print(\"Converting train, val and test texts to csv...\")\n",
        "    train_texts.to_csv('data/train_texts.csv', index=False, header=False)\n",
        "    val_texts.to_csv('data/val_texts.csv', index=False, header=False)\n",
        "    test_texts.to_csv('data/test_texts.csv', index=False, header=False)\n",
        "\n",
        "    if model == 'DistilBert':\n",
        "        test_pred_labels = tune_transformer.run('distilbert-base-uncased', train_texts, val_texts, test_texts, y_train, y_val, y_test)\n",
        "    elif model == 'RoBERTa':\n",
        "        test_pred_labels = tune_transformer.run('roberta-base', train_texts, val_texts, test_texts, y_train, y_val, y_test)\n",
        "    elif model == 'Bert-Large':\n",
        "        test_pred_labels = tune_transformer.run('bert-large-uncased', train_texts, val_texts, test_texts, y_train, y_val, y_test)\n",
        "    elif model == 'XLNet-Large':\n",
        "        test_pred_labels = tune_transformer.run('xlnet-large-cased', train_texts, val_texts, test_texts, y_train, y_val, y_test)\n",
        "    elif model == 'XLM-Roberta-Large':\n",
        "        test_pred_labels = tune_transformer.run('xlm-roberta-large', train_texts, val_texts, test_texts, y_train, y_val, y_test)\n",
        "    \n",
        "    # replace original test labels with predicted labels\n",
        "    shuffled_val['label'] = test_pred_labels\n",
        "\n",
        "    # save the dataframe with predicted labels to a csv file\n",
        "    print(\"Saving predictions to csv...\")\n",
        "    shuffled_val.to_csv('data/prediction_task3.tsv', sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print End Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "print(\"------------------------------------------------\")\n",
        "print(\"End-Time\")\n",
        "# print current time in format: 2019-10-03 13:10:00\n",
        "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
        "print(\"------------------------------------------------\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
