{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print Start time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------\n",
            "Start-Time\n",
            "2024-04-17 13:51:28\n",
            "------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from utils import print_time\n",
        "\n",
        "print_time.print_(\"Start-Time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify modes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "deploying = False\n",
        "if deploying:\n",
        "    print(\"----------Deploying----------\")\n",
        "\n",
        "paraphrase_aug = True\n",
        "traditional_aug = False\n",
        "undersampling = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_checkpoint = 'distilbert-base-uncased'\n",
        "# model_checkpoint = 'roberta-base'\n",
        "# model_checkpoint = 'bert-large-uncased'\n",
        "# model_checkpoint = 'xlnet-base-cased'\n",
        "# model_checkpoint = 'xlnet-large-cased'\n",
        "# model_checkpoint = 'xlm-roberta-large'\n",
        "# model_checkpoint = 'microsoft/deberta-v2-xxlarge'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading data...\n",
            "Data read...\n",
            "           id           keyword  \\\n",
            "0      3u2w5k               run   \n",
            "1      3xbury           outside   \n",
            "2      3y743u  run, swim, climb   \n",
            "3      43bvs7              walk   \n",
            "4      442ap2           outside   \n",
            "...       ...               ...   \n",
            "2395  edvs552              walk   \n",
            "2396   ee31pf           outside   \n",
            "2397  eei3pz3     outside, walk   \n",
            "2398  eek8bpk           outside   \n",
            "2399  eeljxq0           outside   \n",
            "\n",
            "                                                   text  label  \n",
            "0     Afterwards, I want to make a run at young love...      0  \n",
            "1     I've met her and several other girls on a spec...      0  \n",
            "2     Anyway! I'd also like to be able to talk to pe...      0  \n",
            "3     I did and again I failed to utter a word. I go...      0  \n",
            "4     Anyway I ran back inside sort of grunting to a...      0  \n",
            "...                                                 ...    ...  \n",
            "2395   The thought of applying for a job terrified m...      2  \n",
            "2396  I will forever think people will think of me a...      2  \n",
            "2397  5 or 7, I never shit outside of my house. Almo...      2  \n",
            "2398  After a while of being fed up of the cold I th...      2  \n",
            "2399  I feel like I sometimes go into my own bubble ...      2  \n",
            "\n",
            "[2400 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "from utils import preprocessing\n",
        "\n",
        "df, df_test = preprocessing.preprocess_data(deploying=deploying,\n",
        "                                            train_path='data/SMM4H_2024_Task3_Training_1800.csv',\n",
        "                                            val_path='data/SMM4H_2024_Task3_Validation_600.csv',\n",
        "                                            test_path='data/SMM4H_Task3_testposts.csv',\n",
        "                                            model_checkpoint=model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CXNHCTPGiBr"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, val_texts, y_train, y_val = train_test_split(\n",
        "    df['text'], df['label'],\n",
        "    test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "test_texts = df_test['text']\n",
        "if not deploying:\n",
        "    y_test = df_test['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution before cutting:\n",
            " label\n",
            "0    796\n",
            "2    265\n",
            "1    116\n",
            "3     83\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame({'text': train_texts, 'label': y_train})\n",
        "\n",
        "# Contar el número de publicaciones en cada categoría\n",
        "class_counts = train_df['label'].value_counts()\n",
        "print(\"Class distribution before augmenting with paraphrased texts:\\n\", class_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augment train_df by augmented texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paraphrased 1 df:\n",
            "                                                   text  label\n",
            "0    It's the best way for me to get outside at nig...      1\n",
            "1    Take a long walk in nature and write everythin...      1\n",
            "2    If you take a walk, you can start in a place w...      1\n",
            "3    I went outside for the first time in about 8 m...      1\n",
            "4    It would allow me to do more hobbies like park...      1\n",
            "..                                                 ...    ...\n",
            "144  Even if I'm still here and I still feel the an...      1\n",
            "145                     There is a <sep> Go for a walk      1\n",
            "146  You will feel less anxious if you are outside ...      1\n",
            "147  Do you know the word \"fika\" in English, coffee...      1\n",
            "148  Take a walk in the park, small things like tha...      1\n",
            "\n",
            "[149 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if deploying:\n",
        "    paraphrase_path = \"data/augmented_dfs_trainval/\"\n",
        "    aug_path = \"data/traditional_augmentation_trainval/\"\n",
        "else:\n",
        "    paraphrase_path = \"data/augmented_dfs_train/\"\n",
        "    aug_path = \"data/traditional_augmentation_train/\"\n",
        "    \n",
        "if paraphrase_aug:\n",
        "    paraphrased_1_df_1 = pd.read_csv(paraphrase_path + 'Paraphrase1/paraphrased_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
        "    paraphrased_1_df_2 = pd.read_csv(paraphrase_path + 'Paraphrase2/paraphrased_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
        "    paraphrased_1_df_3 = pd.read_csv(paraphrase_path + 'Paraphrase3/paraphrased_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
        "    \n",
        "    paraphrased_2_df_1 = pd.read_csv(paraphrase_path + 'Paraphrase1/paraphrased_class_2.csv', usecols=['text', 'label', 'keyword'])\n",
        "\n",
        "    paraphrased_3_df_1 = pd.read_csv(paraphrase_path + 'Paraphrase1/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
        "    paraphrased_3_df_2 = pd.read_csv(paraphrase_path + 'Paraphrase2/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
        "    paraphrased_3_df_3 = pd.read_csv(paraphrase_path + 'Paraphrase3/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
        "    paraphrased_3_df_4 = pd.read_csv(paraphrase_path + 'Paraphrase4/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
        "\n",
        "    paraphrased_df = pd.concat([paraphrased_1_df_1, paraphrased_1_df_2, paraphrased_1_df_3, paraphrased_2_df_1, paraphrased_3_df_1, paraphrased_3_df_2, paraphrased_3_df_3, paraphrased_3_df_4], ignore_index=True)\n",
        "\n",
        "    # Add keywords to paraphrased dfs\n",
        "    paraphrased_df = preprocessing.add_keywords(paraphrased_df, model_checkpoint)\n",
        "\n",
        "    train_df = pd.concat([train_df, paraphrased_df], ignore_index=True)\n",
        "    \n",
        "if traditional_aug:\n",
        "    punct_df = pd.read_csv(aug_path + 'punct_df.csv', usecols=['text', 'label', 'keyword'])\n",
        "    # punct_df = punct_df.loc[punct_df['label'] != 0]\n",
        "    # punct_df = punct_df.loc[punct_df['label'] != 2]\n",
        "\n",
        "    rnd_del_df = pd.read_csv(aug_path + 'rnd_del_df.csv', usecols=['text', 'label', 'keyword'])\n",
        "    # rnd_del_df = rnd_del_df.loc[rnd_del_df['label'] != 0]\n",
        "    # rnd_del_df = rnd_del_df.loc[rnd_del_df['label'] != 2]\n",
        "\n",
        "    rnd_swap_df = pd.read_csv(aug_path + 'rnd_swap_df.csv', usecols=['text', 'label', 'keyword'])\n",
        "    # rnd_swap_df = rnd_swap_df.loc[rnd_swap_df['label'] != 0]\n",
        "    # rnd_swap_df = rnd_swap_df.loc[rnd_swap_df['label'] != 2]\n",
        "\n",
        "    rnd_insert_df = pd.read_csv(aug_path + 'rnd_insert_df.csv', usecols=['text', 'label', 'keyword'])\n",
        "    # rnd_insert_df = rnd_insert_df.loc[rnd_insert_df['label'] != 0]\n",
        "    # rnd_insert_df = rnd_insert_df.loc[rnd_insert_df['label'] != 2]\n",
        "\n",
        "    aug_df = pd.concat([punct_df, rnd_del_df, rnd_swap_df, rnd_insert_df], ignore_index=True)\n",
        "\n",
        "    # Add keywords to augmented dfs\n",
        "    aug_df = preprocessing.add_keywords(aug_df, model_checkpoint)\n",
        "\n",
        "    # merge df with paraphrased dfs\n",
        "    # train_df = pd.concat([train_df, paraphrased_df, aug_df], ignore_index=True)\n",
        "    train_df = pd.concat([train_df, aug_df], ignore_index=True)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cut Classes to X texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Fgq27NJvF5",
        "outputId": "aed84b8f-48cf-4880-c98b-d7c908ff93ed"
      },
      "outputs": [],
      "source": [
        "if undersampling:\n",
        "    # Size of each class after sampling (Hyperparameter)\n",
        "    # Class 0 has 796 samples and was not augmented\n",
        "    class_size = 1500\n",
        "\n",
        "    # Sample 200 texts from each class (or as many as are available for classes with fewer than 200 examples)\n",
        "    sampled_dfs = []\n",
        "    for label in train_df['label'].unique():\n",
        "        class_sample_size = min(len(train_df[train_df['label'] == label]), class_size)\n",
        "        sampled_dfs.append(train_df[train_df['label'] == label].sample(n=class_sample_size, random_state=42))\n",
        "\n",
        "    # Concatenate the samples to create a balanced training DataFrame\n",
        "    train_df = pd.concat(sampled_dfs, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract texts and labels from train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train texts balanced 0       yeah i was in your boat for 2 years in the end...\n",
            "1       we exchanged numbers and pics proving we are r...\n",
            "2       I have to take both of my dogs for a walk ever...\n",
            "3       as part of my office job, you had to deal with...\n",
            "4       i went to the gaeltacht (irish speaking summer...\n",
            "                              ...                        \n",
            "1773    table tennis, as soon as i start playing, my a...\n",
            "1774    I realize that the inner critic has no power w...\n",
            "1775    and an automated turret placed outside <sep> o...\n",
            "1776    I don't like riding in cars that don't have ti...\n",
            "1777    i got there at 8:55 and decided to just wait t...\n",
            "Name: text, Length: 1778, dtype: object\n",
            "Datatype of y_train <class 'pandas.core.series.Series'>\n",
            "y_train balanced 0       0\n",
            "1       2\n",
            "2       3\n",
            "3       0\n",
            "4       2\n",
            "       ..\n",
            "1773    0\n",
            "1774    1\n",
            "1775    0\n",
            "1776    3\n",
            "1777    0\n",
            "Name: label, Length: 1778, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "shuffled_train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "# Now you can extract the texts and labels\n",
        "train_texts = shuffled_train_df['text']\n",
        "print(\"Train texts balanced\", train_texts)\n",
        "# print datatype of y train values\n",
        "y_train = shuffled_train_df['label']\n",
        "print(\"Datatype of y_train\", type(y_train))\n",
        "print(\"y_train balanced\", y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print train_df class distribution after cutting/before augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar el número de publicaciones en cada categoría\n",
        "class_counts = train_df['label'].value_counts()\n",
        "print(\"Class distribution after cutting:\\n\", class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# df_plot = train_df.copy()\n",
        "\n",
        "# label_mapping = {1: 'positive', 2: 'neutral', 3: 'negative', 0: 'unrelated'}\n",
        "# df_plot['label'] = df_plot['label'].map(label_mapping)\n",
        "\n",
        "# # Contar el número de publicaciones en cada categoría\n",
        "# class_counts = df_plot['label'].value_counts()\n",
        "# print(class_counts)\n",
        "\n",
        "# # Crear un gráfico de barras\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# class_counts.plot(kind='bar')\n",
        "# plt.title('Distribución de clases')\n",
        "# plt.xlabel('Clase')\n",
        "# plt.ylabel('Número de publicaciones')\n",
        "# plt.xticks(rotation=0)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    'epochs': 100,\n",
        "    'batch_size': 16,\n",
        "    'weight_decay': 0.01,\n",
        "    'learning_rate': 2e-6,\n",
        "    'warmup_steps': 1000,\n",
        "    'metric_for_best_model': \"f1\",\n",
        "    'early_stopping_patience': 4,\n",
        "    'max_length': 256,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models import tune_transformer\n",
        "# from models import tune_transformer_accelerate as tune_transformer\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Model:\", model_checkpoint)\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"Converting train, val and test texts to csv...\")\n",
        "train_texts.to_csv('data/train_texts.csv', index=False, header=False)\n",
        "val_texts.to_csv('data/val_texts.csv', index=False, header=False)\n",
        "test_texts.to_csv('data/test_texts.csv', index=False, header=False)\n",
        "\n",
        "if deploying:\n",
        "    test_pred_labels = tune_transformer.run(model_checkpoint, 4,\n",
        "                                            train_texts, val_texts, test_texts,\n",
        "                                            y_train, y_val, y_test=None,\n",
        "                                            hyperparameters=hyperparameters)\n",
        "    \n",
        "    # replace original test labels with predicted labels\n",
        "    df_test['label'] = test_pred_labels\n",
        "\n",
        "    # save the dataframe with predicted labels to a csv file\n",
        "    print(\"Saving predictions to csv...\")\n",
        "    df_test.to_csv('data/prediction_task3.tsv', sep='\\t', index=False)\n",
        "else:\n",
        "    test_pred_labels = tune_transformer.run(model_checkpoint, 4,\n",
        "                                            train_texts, val_texts, test_texts,\n",
        "                                            y_train, y_val, y_test,\n",
        "                                            hyperparameters=hyperparameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print End Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_time.print_(\"End-Time\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
