{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print Start time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------\n",
            "Start-Time\n",
            "2024-04-17 13:51:28\n",
            "------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "print(\"------------------------------------------------\")\n",
        "print(\"Start-Time\")\n",
        "# print current time in format: 2019-10-03 13:10:00\n",
        "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
        "print(\"------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Does df-save-path exist: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Does df-save-path exist:\", os.path.exists('data/augmented_train_dfs'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify modes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "deploying = True\n",
        "final_deploy = False\n",
        "augmenting = False\n",
        "\n",
        "paraphrase_aug = True\n",
        "traditional_aug = True\n",
        "\n",
        "if final_deploy:\n",
        "    print(\"-----------------FINAL DEPLOYMENT-----------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = 'distilbert-base-uncased'\n",
        "# model = 'roberta-base'\n",
        "# model = 'bert-large-uncased'\n",
        "# model = 'xlnet-base-cased'\n",
        "model = 'xlnet-large-cased'\n",
        "# model = 'xlm-roberta-large'\n",
        "# model = 'microsoft/deberta-v2-xxlarge'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading data...\n",
            "Data read...\n",
            "           id           keyword  \\\n",
            "0      3u2w5k               run   \n",
            "1      3xbury           outside   \n",
            "2      3y743u  run, swim, climb   \n",
            "3      43bvs7              walk   \n",
            "4      442ap2           outside   \n",
            "...       ...               ...   \n",
            "2395  edvs552              walk   \n",
            "2396   ee31pf           outside   \n",
            "2397  eei3pz3     outside, walk   \n",
            "2398  eek8bpk           outside   \n",
            "2399  eeljxq0           outside   \n",
            "\n",
            "                                                   text  label  \n",
            "0     Afterwards, I want to make a run at young love...      0  \n",
            "1     I've met her and several other girls on a spec...      0  \n",
            "2     Anyway! I'd also like to be able to talk to pe...      0  \n",
            "3     I did and again I failed to utter a word. I go...      0  \n",
            "4     Anyway I ran back inside sort of grunting to a...      0  \n",
            "...                                                 ...    ...  \n",
            "2395   The thought of applying for a job terrified m...      2  \n",
            "2396  I will forever think people will think of me a...      2  \n",
            "2397  5 or 7, I never shit outside of my house. Almo...      2  \n",
            "2398  After a while of being fed up of the cold I th...      2  \n",
            "2399  I feel like I sometimes go into my own bubble ...      2  \n",
            "\n",
            "[2400 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    return [lemmatizer.lemmatize(word.lower()) for word in words if word.isalnum()]\n",
        "\n",
        "def filter_sentences(row):\n",
        "    # Assuming keywords are separated by commas and possibly spaces\n",
        "    keywords = [lemmatizer.lemmatize(word) for word in row['keyword'].replace(' ', '').split(',')]\n",
        "    text = row['text']\n",
        "    sentences = sent_tokenize(text)\n",
        "    \n",
        "    # Filter sentences that contain at least one lemmatized keyword\n",
        "    filtered_sentences = set()  # Use a set to prevent duplicates\n",
        "    for index, sentence in enumerate(sentences):\n",
        "        words = lemmatize_words(word_tokenize(sentence))\n",
        "        if any(keyword in words for keyword in keywords):\n",
        "            # Add previous sentence if it exists\n",
        "            if index > 0:\n",
        "                filtered_sentences.add(sentences[index - 1])\n",
        "            # Add current sentence\n",
        "            filtered_sentences.add(sentence)\n",
        "            # Add next sentence if it exists\n",
        "            if index < len(sentences) - 1:\n",
        "                filtered_sentences.add(sentences[index + 1])\n",
        "\n",
        "    return ' '.join(sorted(filtered_sentences)) if filtered_sentences else text  # Return original text if no keywords found\n",
        "\n",
        "\n",
        "print(\"Reading data...\")\n",
        "df = pd.read_csv('data/SMM4H_2024_Task3_Training_1800.csv', usecols=['id', 'keyword', 'text', 'label'])\n",
        "if not augmenting:\n",
        "    df_val = pd.read_csv('data/SMM4H_2024_Task3_Validation_600.csv', usecols=['id', 'keyword', 'text', 'label'])\n",
        "print(\"Data read...\")\n",
        "# Keep the validation data apart when deploying\n",
        "if not deploying and not augmenting:\n",
        "    df = pd.concat([df, df_val], ignore_index=True)\n",
        "if final_deploy:\n",
        "    df_test = pd.read_csv('data/SMM4H_Task3_testposts.csv', usecols=['id', 'keyword', 'text'])\n",
        "\n",
        "# Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply the function to filter sentences in the text\n",
        "df['text'] = df.apply(filter_sentences, axis=1)\n",
        "if deploying:\n",
        "    df_val['text'] = df_val.apply(filter_sentences, axis=1)\n",
        "elif final_deploy:\n",
        "    df_test['text'] = df_test.apply(filter_sentences, axis=1)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get separation token by model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sep_token(model):\n",
        "    if model == 'distilbert-base-uncased' or model == 'roberta-base' or model == 'bert-large-uncased' or model == 'microsoft/deberta-v2-xxlarge' or 'albert' in model:\n",
        "        sep_token = '[SEP]'\n",
        "    elif 'xlnet' in model:\n",
        "        sep_token = '<sep>'\n",
        "    elif model == 'xlm-roberta-large':\n",
        "        sep_token = '</s>'\n",
        "    return sep_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add VADER Sentiment to df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "# import nltk\n",
        "# nltk.download('vader_lexicon')\n",
        "\n",
        "# # Initialize the VADER sentiment intensity analyzer\n",
        "# sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# # Function to get the compound sentiment score\n",
        "# def get_vader_sentiment(text):\n",
        "#     return sid.polarity_scores(text)['compound']\n",
        "\n",
        "# def add_vader_sentiment(df_, model):\n",
        "#     sep_token = get_sep_token(model)\n",
        "    \n",
        "#     df_['text'] = df_['text'] + f\" {sep_token} Sentiment score: \" + df_['vader_sentiment'].astype(str)\n",
        "#     df_.drop(columns=['vader_sentiment'], inplace=True)\n",
        "#     return df_\n",
        "\n",
        "# # Apply the function to get the compound sentiment score for each post\n",
        "# df['vader_sentiment'] = df['text'].apply(get_vader_sentiment)\n",
        "# df = add_vader_sentiment(df, model)\n",
        "# if deploying:\n",
        "#     df_val['vader_sentiment'] = df_val['text'].apply(get_vader_sentiment)\n",
        "#     df_val = add_vader_sentiment(df_val, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add Keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_keywords(df_, model):\n",
        "    sep_token = get_sep_token(model)\n",
        "    \n",
        "    df_['text'] = df_['text'] + f\" {sep_token} Keyword: \" + df_['keyword']\n",
        "    df_.drop(columns=['keyword'], inplace=True)\n",
        "    return df_\n",
        "\n",
        "if not augmenting:\n",
        "    df = add_keywords(df, model)\n",
        "    if deploying:\n",
        "        df_val = add_keywords(df_val, model)\n",
        "    elif final_deploy:\n",
        "        df_test = add_keywords(df_test, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQTbkPUSp-Zk"
      },
      "source": [
        "## Clean text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m6qh9SyUqAHJ"
      },
      "outputs": [],
      "source": [
        "# import emoji library\n",
        "import emoji\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    import re\n",
        "    # Perform emoji to text conversion\n",
        "    text = emoji.demojize(text)\n",
        "    # Convert to lowercase\n",
        "    # text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "    # Remove special characters and numbers\n",
        "    # text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "if deploying:\n",
        "    df_val['text'] = df_val['text'].apply(clean_text)\n",
        "elif final_deploy:\n",
        "    df_test['text'] = df_test['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CXNHCTPGiBr"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "if not deploying and not augmenting and not final_deploy:\n",
        "    # ----- Without Keywords (for training) -----\n",
        "    # First, split the data into a training set and a temporary set (which will be further split into validation and test sets)\n",
        "    train_texts, temp_texts, y_train, temp_labels = train_test_split(\n",
        "        df['text'], df['label'],\n",
        "        test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Next, split the temporary set into validation and test sets\n",
        "    val_texts, test_texts, y_val, y_test = train_test_split(\n",
        "        temp_texts, temp_labels,\n",
        "        test_size=0.5, random_state=42\n",
        "    )\n",
        "elif deploying:\n",
        "    train_texts, val_texts, y_train, y_val = train_test_split(\n",
        "        df['text'], df['label'],\n",
        "        test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    test_texts = df_val['text']\n",
        "    y_test = df_val['label']\n",
        "elif augmenting:\n",
        "    train_texts, val_texts, train_keywords, val_keywords, y_train, y_val = train_test_split(\n",
        "    df['text'], df['keyword'], df['label'],\n",
        "    test_size=0.3, random_state=42\n",
        "    )\n",
        "elif final_deploy:\n",
        "    train_texts, val_texts, y_train, y_val = train_test_split(\n",
        "        df['text'], df['label'],\n",
        "        test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    test_texts = df_test['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution before cutting:\n",
            " label\n",
            "0    796\n",
            "2    265\n",
            "1    116\n",
            "3     83\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "if not augmenting:\n",
        "    # ----- Without Keywords (for training) -----\n",
        "    # Create a DataFrame from the training texts and labels\n",
        "    train_df = pd.DataFrame({'text': train_texts, 'label': y_train})\n",
        "else:\n",
        "    train_df = pd.DataFrame({'text': train_texts, 'label': y_train, 'keyword': train_keywords})\n",
        "\n",
        "# Contar el número de publicaciones en cada categoría\n",
        "class_counts = train_df['label'].value_counts()\n",
        "print(\"Class distribution before augmenting with paraphrased texts:\\n\", class_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augment train_df by augmented texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paraphrased 1 df:\n",
            "                                                   text  label\n",
            "0    It's the best way for me to get outside at nig...      1\n",
            "1    Take a long walk in nature and write everythin...      1\n",
            "2    If you take a walk, you can start in a place w...      1\n",
            "3    I went outside for the first time in about 8 m...      1\n",
            "4    It would allow me to do more hobbies like park...      1\n",
            "..                                                 ...    ...\n",
            "144  Even if I'm still here and I still feel the an...      1\n",
            "145                     There is a <sep> Go for a walk      1\n",
            "146  You will feel less anxious if you are outside ...      1\n",
            "147  Do you know the word \"fika\" in English, coffee...      1\n",
            "148  Take a walk in the park, small things like tha...      1\n",
            "\n",
            "[149 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if deploying:\n",
        "    paraphrase_path = \"data/augmented_dfs_train/\"\n",
        "    aug_path = \"data/traditional_augmentation_train/\"\n",
        "elif final_deploy:\n",
        "    paraphrase_path = \"data/augmented_dfs_trainval/\"\n",
        "    aug_path = \"data/traditional_augmentation_trainval/\"\n",
        "\n",
        "if not augmenting:\n",
        "    if paraphrase_aug:\n",
        "        paraphrased_1_df_1 = pd.read_csv(paraphrase_path + 'Paraphrase1/paraphrased_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
        "        paraphrased_1_df_2 = pd.read_csv(paraphrase_path + 'Paraphrase2/paraphrased_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
        "        paraphrased_1_df_3 = pd.read_csv(paraphrase_path + 'Paraphrase3/paraphrased_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
        "        \n",
        "        paraphrased_2_df_1 = pd.read_csv(paraphrase_path + 'Paraphrase1/paraphrased_class_2.csv', usecols=['text', 'label', 'keyword'])\n",
        "\n",
        "        paraphrased_3_df_1 = pd.read_csv(paraphrase_path + 'Paraphrase1/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
        "        paraphrased_3_df_2 = pd.read_csv(paraphrase_path + 'Paraphrase2/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
        "        paraphrased_3_df_3 = pd.read_csv(paraphrase_path + 'Paraphrase3/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
        "        paraphrased_3_df_4 = pd.read_csv(paraphrase_path + 'Paraphrase4/paraphrased_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
        "\n",
        "        paraphrased_df = pd.concat([paraphrased_1_df_1, paraphrased_1_df_2, paraphrased_1_df_3, paraphrased_2_df_1, paraphrased_3_df_1, paraphrased_3_df_2, paraphrased_3_df_3, paraphrased_3_df_4], ignore_index=True)\n",
        "\n",
        "        # Add keywords to paraphrased dfs\n",
        "        paraphrased_df = add_keywords(paraphrased_df, model)\n",
        "\n",
        "        train_df = pd.concat([train_df, paraphrased_df], ignore_index=True)\n",
        "    \n",
        "    if traditional_aug:\n",
        "        punct_df = pd.read_csv(aug_path + 'punct_df.csv', usecols=['text', 'label', 'keyword'])\n",
        "        # punct_df = punct_df.loc[punct_df['label'] != 0]\n",
        "        # punct_df = punct_df.loc[punct_df['label'] != 2]\n",
        "\n",
        "        rnd_del_df = pd.read_csv(aug_path + 'rnd_del_df.csv', usecols=['text', 'label', 'keyword'])\n",
        "        # rnd_del_df = rnd_del_df.loc[rnd_del_df['label'] != 0]\n",
        "        # rnd_del_df = rnd_del_df.loc[rnd_del_df['label'] != 2]\n",
        "\n",
        "        rnd_swap_df = pd.read_csv(aug_path + 'rnd_swap_df.csv', usecols=['text', 'label', 'keyword'])\n",
        "        # rnd_swap_df = rnd_swap_df.loc[rnd_swap_df['label'] != 0]\n",
        "        # rnd_swap_df = rnd_swap_df.loc[rnd_swap_df['label'] != 2]\n",
        "\n",
        "        rnd_insert_df = pd.read_csv(aug_path + 'rnd_insert_df.csv', usecols=['text', 'label', 'keyword'])\n",
        "        # rnd_insert_df = rnd_insert_df.loc[rnd_insert_df['label'] != 0]\n",
        "        # rnd_insert_df = rnd_insert_df.loc[rnd_insert_df['label'] != 2]\n",
        "\n",
        "        aug_df = pd.concat([punct_df, rnd_del_df, rnd_swap_df, rnd_insert_df], ignore_index=True)\n",
        "\n",
        "        # Add keywords to augmented dfs\n",
        "        aug_df = add_keywords(aug_df, model)\n",
        "\n",
        "        # merge df with paraphrased dfs\n",
        "        # train_df = pd.concat([train_df, paraphrased_df, aug_df], ignore_index=True)\n",
        "        train_df = pd.concat([train_df, aug_df], ignore_index=True)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augment train_df by backtranslated texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if not augmenting:\n",
        "\n",
        "    ## ----------- Comment code if augmenting data ------------\n",
        "\n",
        "    # back_translated_1_df = pd.read_csv('data/augmented_train_dfs/backtranslated_class_1.csv', usecols=['text', 'label'])\n",
        "    # back_translated_3_df = pd.read_csv('data/augmented_train_dfs/backtranslated_class_3.csv', usecols=['text', 'label'])\n",
        "\n",
        "    # # merge df with backtranslated dfs\n",
        "    # train_df = pd.concat([train_df, back_translated_1_df, back_translated_3_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augment data by ChatGPT 4 generated texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chatGPT4_1_df:\n",
            "                                                  text  label\n",
            "0   The sense of freedom I feel while kayaking on ...      1\n",
            "1   Engaging in outdoor photography in the forest ...      1\n",
            "2   Participating in a community picnic in the par...      1\n",
            "3   The tranquility of sitting on a bench in the g...      1\n",
            "4   The adventure of exploring new hiking trails i...      1\n",
            "5   The simple pleasure of feeding ducks at the la...      1\n",
            "6   Taking part in a group meditation session in t...      1\n",
            "7   The beauty of a sunrise at the beach fills me ...      1\n",
            "8   The challenge of participating in a forest tra...      1\n",
            "9   The peace and quiet of a countryside retreat a...      1\n",
            "10  Volunteering at an animal shelter in the garde...      1\n",
            "11  The sense of achievement I feel after completi...      1\n",
            "12  The soothing sound of waves at the beach, comb...      1\n",
            "13  The joy of participating in a community garden...      1\n",
            "14  The experience of watching a sunset over the l...      1\n",
            "15  The excitement of discovering new species duri...      1\n",
            "16  Practicing tai chi in the park, surrounded by ...      1\n",
            "17  The thrill of windsurfing on the river provide...      1\n",
            "18  The sense of community I feel during an outdoo...      1\n",
            "19  The serenity of watching fish swim in a garden...      1\n",
            "20  Taking a stroll in the park, I find solace in ...      1\n",
            "21  The rhythmic sound of the waves at the beach h...      1\n",
            "22  Exploring the forest, I feel a deep connection...      1\n",
            "23  The majestic view of the mountains instills a ...      1\n",
            "24  Sitting by the lake, I feel a sense of calm wa...      1\n",
            "25  The vibrant colors of the flowers in the garde...      1\n",
            "26  Joining a group of friends for a picnic in the...      1\n",
            "27  Building sandcastles on the beach, I feel a ch...      1\n",
            "28  Hiking through the forest, I focus on the beau...      1\n",
            "29  Reaching the summit of a mountain, I feel a se...      1\n",
            "30  Kayaking on the lake, I enjoy the peaceful sol...      1\n",
            "31  Tending to my garden, I find that the repetiti...      1\n",
            "32  Watching children play in the park, I'm remind...      1\n",
            "33  Walking along the beach, I feel a sense of fre...      1\n",
            "34  The forest is my sanctuary, where I can be alo...      1\n",
            "35  The crisp mountain air invigorates me, giving ...      1\n",
            "36  The tranquility of the lake at dawn provides a...      1\n",
            "37  The fragrance of fresh blooms in the garden ha...      1\n",
            "38  Participating in a yoga session in the park, I...      1\n",
            "39  The vastness of the ocean at the beach reminds...      1\n"
          ]
        }
      ],
      "source": [
        "# if not augmenting:\n",
        "#     ## ----------- Comment code if augmenting data ------------\n",
        "\n",
        "#     chatGPT4_1_df = pd.read_csv('data/augmented_train_dfs/ChatGPT4_texts_class_1.csv', usecols=['text', 'label', 'keyword'])\n",
        "#     chatGPT4_3_df = pd.read_csv('data/augmented_train_dfs/ChatGPT4_texts_class_3.csv', usecols=['text', 'label', 'keyword'])\n",
        "\n",
        "#     # Add keywords to ChatGPT4 dfs\n",
        "#     chatGPT4_1_df = add_keywords(chatGPT4_1_df, model)\n",
        "#     chatGPT4_3_df = add_keywords(chatGPT4_3_df, model)\n",
        "\n",
        "#     # Print chatGPT4 dfs\n",
        "#     print(\"chatGPT4_1_df:\\n\", chatGPT4_1_df)\n",
        "\n",
        "#     # merge df with paraphrased dfs\n",
        "#     train_df = pd.concat([train_df, chatGPT4_1_df, chatGPT4_3_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cut Classes to X texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Fgq27NJvF5",
        "outputId": "aed84b8f-48cf-4880-c98b-d7c908ff93ed"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "if not augmenting:\n",
        "\n",
        "    # Size of each class after sampling (Hyperparameter)\n",
        "    # Class 0 has 796 samples and was not augmented\n",
        "    class_size = 1500\n",
        "\n",
        "    # Sample 200 texts from each class (or as many as are available for classes with fewer than 200 examples)\n",
        "    sampled_dfs = []\n",
        "    for label in train_df['label'].unique():\n",
        "        class_sample_size = min(len(train_df[train_df['label'] == label]), class_size)\n",
        "        sampled_dfs.append(train_df[train_df['label'] == label].sample(n=class_sample_size, random_state=42))\n",
        "\n",
        "    # Concatenate the samples to create a balanced training DataFrame\n",
        "    train_df = pd.concat(sampled_dfs, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract texts and labels from train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train texts balanced 0       yeah i was in your boat for 2 years in the end...\n",
            "1       we exchanged numbers and pics proving we are r...\n",
            "2       I have to take both of my dogs for a walk ever...\n",
            "3       as part of my office job, you had to deal with...\n",
            "4       i went to the gaeltacht (irish speaking summer...\n",
            "                              ...                        \n",
            "1773    table tennis, as soon as i start playing, my a...\n",
            "1774    I realize that the inner critic has no power w...\n",
            "1775    and an automated turret placed outside <sep> o...\n",
            "1776    I don't like riding in cars that don't have ti...\n",
            "1777    i got there at 8:55 and decided to just wait t...\n",
            "Name: text, Length: 1778, dtype: object\n",
            "Datatype of y_train <class 'pandas.core.series.Series'>\n",
            "y_train balanced 0       0\n",
            "1       2\n",
            "2       3\n",
            "3       0\n",
            "4       2\n",
            "       ..\n",
            "1773    0\n",
            "1774    1\n",
            "1775    0\n",
            "1776    3\n",
            "1777    0\n",
            "Name: label, Length: 1778, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "if not augmenting:\n",
        "    shuffled_train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    # Now you can extract the texts and labels\n",
        "    train_texts = shuffled_train_df['text']\n",
        "    print(\"Train texts balanced\", train_texts)\n",
        "    # print datatype of y train values\n",
        "    y_train = shuffled_train_df['label']\n",
        "    print(\"Datatype of y_train\", type(y_train))\n",
        "    print(\"y_train balanced\", y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print train_df class distribution after cutting/before augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar el número de publicaciones en cada categoría\n",
        "class_counts = train_df['label'].value_counts()\n",
        "print(\"Class distribution after cutting:\\n\", class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# df_plot = train_df.copy()\n",
        "\n",
        "# label_mapping = {1: 'positive', 2: 'neutral', 3: 'negative', 0: 'unrelated'}\n",
        "# df_plot['label'] = df_plot['label'].map(label_mapping)\n",
        "\n",
        "# # Contar el número de publicaciones en cada categoría\n",
        "# class_counts = df_plot['label'].value_counts()\n",
        "# print(class_counts)\n",
        "\n",
        "# # Crear un gráfico de barras\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# class_counts.plot(kind='bar')\n",
        "# plt.title('Distribución de clases')\n",
        "# plt.xlabel('Clase')\n",
        "# plt.ylabel('Número de publicaciones')\n",
        "# plt.xticks(rotation=0)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backtranslate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "backtranslate = False\n",
        "# Save the augmented training dataframe to a CSV file\n",
        "train_df_path = 'data/augmented_train_dfs/train_df_plus_backtranslated_class_1_3.csv'\n",
        "\n",
        "if backtranslate:\n",
        "\n",
        "    from utils import backtranslation\n",
        "\n",
        "    for label in {1, 3}:\n",
        "        print(f\"Backtranslating class {label}...\")\n",
        "        # Backtranslate and augment the data for underrepresented classes\n",
        "        selected_texts = train_df[train_df['label'] == label]['text']\n",
        "        selected_keywords = train_df[train_df['label'] == label]['keyword']\n",
        "        print(f\"length texts of label {label}\", len(selected_texts))\n",
        "        augmented_texts = backtranslation.backtranslate_t5(selected_texts.to_list(), selected_keywords.to_list())\n",
        "        augmented_df = pd.DataFrame({'text': augmented_texts, 'label': [label] * len(augmented_texts)})\n",
        "        augmented_df.to_csv(f'data/augmented_train_dfs/backtranslated_t5_class_{label}.csv', index=False)\n",
        "        train_df = pd.concat([train_df, augmented_df])\n",
        "\n",
        "    # Check the new class distribution after backtranslation\n",
        "    print(\"Class distribution after backtranslation:\", train_df['label'].value_counts())\n",
        "\n",
        "    train_df.to_csv(train_df_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paraphrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paraphrase = False\n",
        "# Save the augmented training dataframe to a CSV file\n",
        "# train_df_path = 'data/augmented_train_dfs/train_df_plus_paraphased_class_1_3.csv'\n",
        "\n",
        "if paraphrase:\n",
        "\n",
        "    from utils import paraphrase_humarin\n",
        "\n",
        "    for label in {1, 2, 3}:\n",
        "        print(f\"Paraphrasing class {label}...\")\n",
        "        # Paraphrase and augment the data for underrepresented classes\n",
        "        selected_texts = train_df.loc[train_df['label'] == label, 'text']\n",
        "        selected_keywords = train_df.loc[train_df['label'] == label, 'keyword']\n",
        "        print(f\"length texts of label {label}\", len(selected_texts))\n",
        "        augmented_texts = paraphrase_humarin.paraphrase(selected_texts.to_list())\n",
        "        # augmented_texts = [[\"t1\", \"t2\", \"t3\", \"t4\"], [\"t12\", \"t22\", \"t32\", \"t42\"]]\n",
        "        for i in range(len(augmented_texts[0])):\n",
        "            print(\"i\", i)\n",
        "            curr_texts = [augmented_texts[j][i] for j in range(len(augmented_texts))]\n",
        "            print(curr_texts)\n",
        "            augmented_df = pd.DataFrame({'text': curr_texts, 'label': [label] * len(curr_texts), 'keyword': selected_keywords.to_list()})\n",
        "            # augmented_df = pd.DataFrame({'text': curr_texts, 'label': [label] * len(curr_texts)})\n",
        "            augmented_df.to_csv(f'data/augmented_dfs_train/Paraphrase{i+1}/paraphrased_class_{label}.csv', index=False)\n",
        "        # train_df = pd.concat([train_df, augmented_df])\n",
        "\n",
        "    # Check the new class distribution after paraphrasing\n",
        "    # print(\"Class distribution after paraphrasing:\", train_df['label'].value_counts())\n",
        "\n",
        "    # train_df.to_csv(train_df_path, index=False)\n",
        "    print(\"\\nDone paraphrasing\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Traditional Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if augmenting:\n",
        "\n",
        "    from utils import punct_insertion, random_deletion, random_swap, random_insertion\n",
        "\n",
        "    print(\"Starting traditional augmentation...\")\n",
        "\n",
        "    train_df['text_punct_insertion'] = train_df['text'].apply(punct_insertion.insert_punctuation)\n",
        "    train_df['text_random_deletion'] = train_df['text'].apply(random_deletion.rnd_del)\n",
        "    train_df['text_random_swap'] = train_df['text'].apply(random_swap.rnd_swap)\n",
        "    train_df['text_random_insertion'] = train_df['text'].apply(random_insertion.rnd_insert)\n",
        "\n",
        "    punct_df = train_df[['text_punct_insertion', 'label', 'keyword']].copy()\n",
        "    # rename text_punct_insertion to text\n",
        "    punct_df.rename(columns={'text_punct_insertion': 'text'}, inplace=True)\n",
        "\n",
        "    rnd_del_df = train_df[['text_random_deletion', 'label', 'keyword']].copy()\n",
        "    # rename text_random_deletion to text\n",
        "    rnd_del_df.rename(columns={'text_random_deletion': 'text'}, inplace=True)\n",
        "\n",
        "    rnd_swap_df = train_df[['text_random_swap', 'label', 'keyword']].copy()\n",
        "    # rename text_random_swap to text\n",
        "    rnd_swap_df.rename(columns={'text_random_swap': 'text'}, inplace=True)\n",
        "\n",
        "    rnd_insert_df = train_df[['text_random_insertion', 'label', 'keyword']].copy()\n",
        "    # rename text_random_insertion to text\n",
        "    rnd_insert_df.rename(columns={'text_random_insertion': 'text'}, inplace=True)\n",
        "\n",
        "    # Save the augmented training dataframe to a CSV file\n",
        "    punct_df.to_csv('data/traditional_augmentation_train/punct_df1.csv', index=False)\n",
        "    rnd_del_df.to_csv('data/traditional_augmentation_train/rnd_del_df1.csv', index=False)\n",
        "    rnd_swap_df.to_csv('data/traditional_augmentation_train/rnd_swap_df1.csv', index=False)\n",
        "    rnd_insert_df.to_csv('data/traditional_augmentation_train/rnd_insert_df1.csv', index=False)\n",
        "\n",
        "    print(\"Traditional augmentation done...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models import tune_transformer\n",
        "# from models import tune_transformer_accelerate as tune_transformer\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Model:\", model)\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "if not augmenting:\n",
        "\n",
        "    print(\"Converting train, val and test texts to csv...\")\n",
        "    train_texts.to_csv('data/train_texts.csv', index=False, header=False)\n",
        "    val_texts.to_csv('data/val_texts.csv', index=False, header=False)\n",
        "    test_texts.to_csv('data/test_texts.csv', index=False, header=False)\n",
        "\n",
        "    test_pred_labels = tune_transformer.run(model, 4, train_texts, val_texts, test_texts, y_train, y_val, y_test)\n",
        "    \n",
        "    if final_deploy:\n",
        "        # replace original test labels with predicted labels\n",
        "        df_test['label'] = test_pred_labels\n",
        "\n",
        "        # save the dataframe with predicted labels to a csv file\n",
        "        print(\"Saving predictions to csv...\")\n",
        "        df_test.to_csv('data/prediction_task3.tsv', sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print End Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "print(\"------------------------------------------------\")\n",
        "print(\"End-Time\")\n",
        "# print current time in format: 2019-10-03 13:10:00\n",
        "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
        "print(\"------------------------------------------------\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
