{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_time\n",
    "\n",
    "print_time.print_(\"Start-Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploying = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = 'distilbert-base-uncased'\n",
    "# model_checkpoint = 'roberta-base'\n",
    "# model_checkpoint = 'bert-large-uncased'\n",
    "# model_checkpoint = 'xlnet-base-cased'\n",
    "model_checkpoint = 'xlnet-large-cased'\n",
    "# model_checkpoint = 'xlm-roberta-large'\n",
    "# model_checkpoint = 'microsoft/deberta-v2-xxlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocessing\n",
    "\n",
    "df, df_test = preprocessing.preprocess_data(deploying=deploying,\n",
    "                                            train_path='data/SMM4H_2024_Task3_Training_1800.csv',\n",
    "                                            val_path='data/SMM4H_2024_Task3_Validation_600.csv',\n",
    "                                            test_path='data/SMM4H_Task3_testposts.csv',\n",
    "                                            model_checkpoint=model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, y_train, y_val = train_test_split(\n",
    "    df['text'], df['label'],\n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "test_texts = df_test['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'text': train_texts, 'label': y_train})\n",
    "\n",
    "# Contar el número de publicaciones en cada categoría\n",
    "class_counts = train_df['label'].value_counts()\n",
    "print(\"Class distribution before augmenting with paraphrased texts:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtranslate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtranslate = False\n",
    "\n",
    "if backtranslate:\n",
    "\n",
    "    from utils import backtranslation\n",
    "\n",
    "    for label in {1, 3}:\n",
    "        print(f\"Backtranslating class {label}...\")\n",
    "        # Backtranslate and augment the data for underrepresented classes\n",
    "        selected_texts = train_df[train_df['label'] == label]['text']\n",
    "        selected_keywords = train_df[train_df['label'] == label]['keyword']\n",
    "        print(f\"length texts of label {label}\", len(selected_texts))\n",
    "        augmented_texts = backtranslation.backtranslate_t5(selected_texts.to_list(), selected_keywords.to_list())\n",
    "        augmented_df = pd.DataFrame({'text': augmented_texts, 'label': [label] * len(augmented_texts)})\n",
    "        augmented_df.to_csv(f'data/augmented_train_dfs/backtranslated_t5_class_{label}.csv', index=False)\n",
    "        train_df = pd.concat([train_df, augmented_df])\n",
    "\n",
    "    # Check the new class distribution after backtranslation\n",
    "    print(\"Class distribution after backtranslation:\", train_df['label'].value_counts())\n",
    "\n",
    "    # Save the augmented training dataframe to a CSV file\n",
    "    train_df_path = 'data/augmented_train_dfs/train_df_plus_backtranslated_class_1_3.csv'\n",
    "    train_df.to_csv(train_df_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase = False\n",
    "# Save the augmented training dataframe to a CSV file\n",
    "# train_df_path = 'data/augmented_train_dfs/train_df_plus_paraphased_class_1_3.csv'\n",
    "\n",
    "if paraphrase:\n",
    "\n",
    "    from utils import paraphrase_humarin\n",
    "\n",
    "    for label in {1, 2, 3}:\n",
    "        print(f\"Paraphrasing class {label}...\")\n",
    "        # Paraphrase and augment the data for underrepresented classes\n",
    "        selected_texts = train_df.loc[train_df['label'] == label, 'text']\n",
    "        selected_keywords = train_df.loc[train_df['label'] == label, 'keyword']\n",
    "        print(f\"length texts of label {label}\", len(selected_texts))\n",
    "        augmented_texts = paraphrase_humarin.paraphrase(selected_texts.to_list())\n",
    "        # augmented_texts = [[\"t1\", \"t2\", \"t3\", \"t4\"], [\"t12\", \"t22\", \"t32\", \"t42\"]]\n",
    "        for i in range(len(augmented_texts[0])):\n",
    "            print(\"i\", i)\n",
    "            curr_texts = [augmented_texts[j][i] for j in range(len(augmented_texts))]\n",
    "            print(curr_texts)\n",
    "            augmented_df = pd.DataFrame({'text': curr_texts, 'label': [label] * len(curr_texts), 'keyword': selected_keywords.to_list()})\n",
    "            # augmented_df = pd.DataFrame({'text': curr_texts, 'label': [label] * len(curr_texts)})\n",
    "            augmented_df.to_csv(f'data/augmented_dfs_train/Paraphrase{i+1}/paraphrased_class_{label}.csv', index=False)\n",
    "        # train_df = pd.concat([train_df, augmented_df])\n",
    "\n",
    "    # Check the new class distribution after paraphrasing\n",
    "    # print(\"Class distribution after paraphrasing:\", train_df['label'].value_counts())\n",
    "\n",
    "    # train_df.to_csv(train_df_path, index=False)\n",
    "    print(\"\\nDone paraphrasing\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmenting:\n",
    "\n",
    "    from utils import punct_insertion, random_deletion, random_swap, random_insertion\n",
    "\n",
    "    print(\"Starting traditional augmentation...\")\n",
    "\n",
    "    train_df['text_punct_insertion'] = train_df['text'].apply(punct_insertion.insert_punctuation)\n",
    "    train_df['text_random_deletion'] = train_df['text'].apply(random_deletion.rnd_del)\n",
    "    train_df['text_random_swap'] = train_df['text'].apply(random_swap.rnd_swap)\n",
    "    train_df['text_random_insertion'] = train_df['text'].apply(random_insertion.rnd_insert)\n",
    "\n",
    "    punct_df = train_df[['text_punct_insertion', 'label', 'keyword']].copy()\n",
    "    # rename text_punct_insertion to text\n",
    "    punct_df.rename(columns={'text_punct_insertion': 'text'}, inplace=True)\n",
    "\n",
    "    rnd_del_df = train_df[['text_random_deletion', 'label', 'keyword']].copy()\n",
    "    # rename text_random_deletion to text\n",
    "    rnd_del_df.rename(columns={'text_random_deletion': 'text'}, inplace=True)\n",
    "\n",
    "    rnd_swap_df = train_df[['text_random_swap', 'label', 'keyword']].copy()\n",
    "    # rename text_random_swap to text\n",
    "    rnd_swap_df.rename(columns={'text_random_swap': 'text'}, inplace=True)\n",
    "\n",
    "    rnd_insert_df = train_df[['text_random_insertion', 'label', 'keyword']].copy()\n",
    "    # rename text_random_insertion to text\n",
    "    rnd_insert_df.rename(columns={'text_random_insertion': 'text'}, inplace=True)\n",
    "\n",
    "    # Save the augmented training dataframe to a CSV file\n",
    "    punct_df.to_csv('data/traditional_augmentation_train/punct_df1.csv', index=False)\n",
    "    rnd_del_df.to_csv('data/traditional_augmentation_train/rnd_del_df1.csv', index=False)\n",
    "    rnd_swap_df.to_csv('data/traditional_augmentation_train/rnd_swap_df1.csv', index=False)\n",
    "    rnd_insert_df.to_csv('data/traditional_augmentation_train/rnd_insert_df1.csv', index=False)\n",
    "\n",
    "    print(\"Traditional augmentation done...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
